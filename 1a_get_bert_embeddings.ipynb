{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0d5254-cdc4-49af-a0cf-44ee33f52e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "\n",
    "## set up tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5375eea-afe7-4a79-babd-a430394403db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11e2f9a-f73c-41f3-b15b-8e763870a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model on GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "print('model on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b382ec41-d65c-4f75-ae30-0a78362110ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wos46985'\n",
    "base_dir = '../../data/WOS/'\n",
    "data_file = base_dir+'Meta-data/Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a8447-f359-43d3-b20c-3b74ad2a75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Data: 46985\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "print('Num Data:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa08f0b6-2b8b-49e4-8572-69ff012e4170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y</th>\n",
       "      <th>Domain</th>\n",
       "      <th>area</th>\n",
       "      <th>keywords</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>CS</td>\n",
       "      <td>Symbolic computation</td>\n",
       "      <td>(2+1)-dimensional non-linear optical waves; e...</td>\n",
       "      <td>\"(2 + 1)-dimensional non-linear optical waves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Alzheimer's Disease</td>\n",
       "      <td>Aging; Tau; Amyloid; PET; Alzheimer's disease...</td>\n",
       "      <td>\"(beta-amyloid (A beta) and tau pathology beco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y1  Y2   Y    Domain                     area  \\\n",
       "0   0  12  12       CS    Symbolic computation     \n",
       "1   5   2  74  Medical     Alzheimer's Disease     \n",
       "\n",
       "                                            keywords  \\\n",
       "0   (2+1)-dimensional non-linear optical waves; e...   \n",
       "1   Aging; Tau; Amyloid; PET; Alzheimer's disease...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  \"(2 + 1)-dimensional non-linear optical waves ...  \n",
       "1  \"(beta-amyloid (A beta) and tau pathology beco...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5cd35c-91c4-4a1e-a592-bea089cbeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, method = 'pool'):\n",
    "    tokens_train = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_length=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    train_seq = tokens_train['input_ids']\n",
    "    train_mask = tokens_train['attention_mask']\n",
    "    train_y = torch.tensor([0]*len(texts))\n",
    "\n",
    "    # define a batch size\n",
    "    batch_size = 20\n",
    "    # wrap tensors\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "    # dataLoader for train set\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    \n",
    "    output_reps_pool = []\n",
    "\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        attention_mask = mask.cpu()\n",
    "        base_output = preds.last_hidden_state.detach().cpu()\n",
    "\n",
    "        if method=='pool':\n",
    "            # Mean Pool and ignore Padding\n",
    "            base_output[attention_mask==0, :] = float('nan')\n",
    "            output_representation = torch.div(torch.nansum(base_output, axis=1), torch.sum(attention_mask==1, axis=1,  keepdim=True))\n",
    "        elif method=='cls':\n",
    "            output_representation = base_output[:,0,:]\n",
    "        output_reps_pool.append(output_representation)\n",
    "        \n",
    "    output_reps_df = torch.vstack(output_reps_pool).numpy()\n",
    "    print('Embeddings Shape:',output_reps_df.shape)\n",
    "    \n",
    "    return output_reps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548bc953-a32a-433b-a9f3-ad71d49652de",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'pool'\n",
    "text_embedding_dir = '../data/'+dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4de7aaf-3d2a-499c-96f5-d4bb0c70aecb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: (46985, 768)\n",
      "Saving Embeddings...\n"
     ]
    }
   ],
   "source": [
    "texts = df.Abstract.tolist()\n",
    "\n",
    "text_embeddings = get_bert_embeddings(texts, method)\n",
    "text_embedding_file = text_embedding_dir+'/roberta-embedding-'+method+'.pkl'\n",
    "\n",
    "if text_embeddings.shape[0] == len(texts):\n",
    "    print('Saving Embeddings...')\n",
    "    pickle.dump(text_embeddings, open(text_embedding_file, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
